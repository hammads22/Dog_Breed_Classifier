# -*- coding: utf-8 -*-
"""AI System Eng - Dog Breed Detection

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zfcRdUh3lNkXM8cIVyNGROqGn2mOjiZW
"""

!pip install roboflow ultralytics matplotlib opencv-python --quiet

import os
import shutil
from collections import Counter, defaultdict

import cv2
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import yaml
from roboflow import Roboflow

rf = Roboflow(api_key="eGsEbW5nGpFjTU1Ml5Th")
project = rf.workspace("igor-romanica-gmail-com").project("stanford-dogs-0pff9")
version = project.version(3)
dataset = version.download("yolov8")

image_dir = '/content/Stanford-dogs-3/train/images'  # Cartella con le immagini
label_dir = '/content/Stanford-dogs-3/train/labels'  # Cartella con i file di etichette

with open('/content/Stanford-dogs-3/data.yaml', 'r') as f:
    data_yaml = yaml.safe_load(f)

class_names = data_yaml['names']

image_files = sorted([f for f in os.listdir(image_dir) if f.endswith('.jpg') or f.endswith('.png')])
label_files = sorted([f for f in os.listdir(label_dir) if f.endswith('.txt')])

# Print a few samples
print(f"Number of images: {len(image_files)}")
print(f"Number of labels: {len(label_files)}")

# Get the class count
class_count = len(class_names)

# Print the class count
print(f"Number of classes: {class_count}")

# Print the class names
print("Class names:")
for class_name in class_names:
    print(class_name)



def count_class_distribution(label_path, class_names):
    class_counts = Counter()

    # Iterate over label files and count each class occurrence
    label_files = sorted([f for f in os.listdir(label_path) if f.endswith('.txt')])

    for label_file in label_files:
        with open(os.path.join(label_path, label_file), 'r') as f:
            annotations = f.readlines()
            for annot in annotations:
                class_idx = int(annot.split()[0])  # Get the class index
                class_counts[class_idx] += 1  # Increment the count for this class

    # Convert class indices to class names
    sorted_class_counts = sorted(class_counts.items(), key=lambda x: x[1], reverse=True)
    sorted_class_names = [class_names[i] for i, _ in sorted_class_counts]
    sorted_counts = [count for _, count in sorted_class_counts]

    return sorted_class_names, sorted_counts

# Path to label files
label_path = '/content/Stanford-dogs-3/train/labels'

# Get the sorted class distribution
sorted_class_names, sorted_counts = count_class_distribution(label_path, class_names)

# Plot the sorted class distribution
plt.figure(figsize=(10, 6))
plt.bar(sorted_class_names, sorted_counts, color='skyblue')
plt.xticks(rotation=45, ha='right')
plt.title('Sorted Class Distribution')
plt.xlabel('Class Label')
plt.ylabel('Frequency')
plt.show()



import os
import matplotlib.pyplot as plt
import cv2

def display_one_image_per_class_with_bboxes(image_dir, label_dir, class_names):
    """Displays one image per class with bounding boxes and labels.

    Args:
        image_dir: Path to the directory containing images.
        label_dir: Path to the directory containing label files.
        class_names: List of class names.
    """
    images_displayed = {class_name: False for class_name in class_names}

    for filename in os.listdir(image_dir):
        if filename.endswith(('.jpg', '.png')):
            image_path = os.path.join(image_dir, filename)
            label_path = os.path.join(label_dir, filename[:-4] + '.txt')

            if os.path.exists(label_path):
                with open(label_path, 'r') as f:
                    img = cv2.imread(image_path)
                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                    height, width, _ = img.shape

                    for line in f:
                        class_index, x_center, y_center, w, h = map(float, line.split())
                        class_name = class_names[int(class_index)]

                        if not images_displayed[class_name]:
                            # Calculate bounding box coordinates
                            x1 = int((x_center - w / 2) * width)
                            y1 = int((y_center - h / 2) * height)
                            x2 = int((x_center + w / 2) * width)
                            y2 = int((y_center + h / 2) * height)

                            # Draw bounding box and label
                            cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)  # Green box
                            cv2.putText(img, class_name, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)

                            plt.imshow(img)
                            plt.title(f"Class: {class_name}")
                            plt.axis('off')
                            plt.show()
                            images_displayed[class_name] = True
                            break  # Break after displaying one image for the class

# Call the function to display images with bounding boxes
display_one_image_per_class_with_bboxes(image_dir, label_dir, class_names)



from collections import defaultdict
import os
import pandas as pd

def bounding_box_statistics(copy_label_dir):
    """
    Calculates the statistics of bounding boxes for each class and returns a summary.
    """
    bbox_stats = defaultdict(list)  # Stores statistics for each class
    label_files = sorted([f for f in os.listdir(copy_label_dir) if f.endswith('.txt')])

    for label_file in label_files:
        label_path = os.path.join(copy_label_dir, label_file)

        with open(label_path, 'r') as f:
            labels = f.readlines()

        for label in labels:
            class_idx, x_center, y_center, width, height = map(float, label.strip().split())
            bbox_area = width * height
            bbox_stats[class_idx].append(bbox_area)

    # Calculate statistics
    stats_summary = {
        class_idx: {
            'Count': len(areas),
            'Mean': sum(areas) / len(areas) if areas else 0,
            'Min': min(areas) if areas else 0,
            'Max': max(areas) if areas else 0
        }
        for class_idx, areas in sorted(bbox_stats.items())  # Sort by class index
    }

    # Convert the summary to a DataFrame for tabular display
    stats_df = pd.DataFrame(stats_summary).T
    stats_df.index.name = 'Class'
    return stats_df

# Run the statistics calculation
stats = bounding_box_statistics(label_dir)

# Display the statistics as a sorted table
print(stats)

bbox_area = []
bbox_classes = []
bbox_features = []



for label_file in label_files:
    with open(os.path.join(label_dir, label_file), 'r') as f:
        labels = f.readlines()
        for label in labels:
            parts = label.strip().split()
            if len(parts) == 5:
                class_idx = int(parts[0])
                x_center = float(parts[1])
                y_center = float(parts[2])
                width = float(parts[3])
                height = float(parts[4])
                area = width * height
                bbox_area.append(area)  # Aggiunge l'area alla lista
                bbox_classes.append(class_idx)
                bbox_features.append([x_center, y_center, width, height, area])

print(f"Bounding boxes number: {len(bbox_area)}")

mean_area = np.mean(bbox_area)
median_area = np.median(bbox_area)
std_area = np.std(bbox_area)
min_area = np.min(bbox_area)
max_area = np.max(bbox_area)

print(f"Mean: {mean_area}, Median: {median_area}, Standard Deviation: {std_area}")
print(f"Min: {min_area}, Max: {max_area}")

plt.figure(figsize=(10, 6))
plt.hist(bbox_area, bins=50, color='skyblue')
plt.title("Distribution of Bounding Box Areas")
plt.xlabel("Area")
plt.ylabel("Frequency")
plt.show()

"""# Data Cleaning"""

# Define the paths to the original folders and the new cloned folders
base_path = '/content/Stanford-dogs-3'  # Change this to your dataset's root directory
folders_to_clone = ['train', 'test', 'valid']

# Clone each folder
for folder in folders_to_clone:
    original_folder = os.path.join(base_path, folder)
    cloned_folder = os.path.join(base_path, f"{folder}_model")

    # Create the clone folder if it doesn't exist
    if not os.path.exists(cloned_folder):
        os.makedirs(cloned_folder)

    # Copy all files and subdirectories to the clone
    for item in os.listdir(original_folder):
        source = os.path.join(original_folder, item)
        destination = os.path.join(cloned_folder, item)
        if os.path.isdir(source):
            shutil.copytree(source, destination)
        else:
            shutil.copy2(source, destination)

print("Cloning complete: created train_model, test_model, and valid_model folders with all files.")

model_image_dir = '/content/Stanford-dogs-3/train_model/images'
model_label_dir = '/content/Stanford-dogs-3/train_model/labels'

with open('/content/Stanford-dogs-3/data.yaml', 'r') as f:
    data_yaml = yaml.safe_load(f)
class_names = data_yaml['names']

def validate_dataset_coordinates(labels_path):
    invalid_files = []
    for label_file in os.listdir(labels_path):
        with open(os.path.join(labels_path, label_file), 'r') as f:
            for line in f:
                class_idx, x_center, y_center, width, height = map(float, line.strip().split())
                if not all(0 < coord <= 1 for coord in [x_center, y_center, width, height]):
                    invalid_files.append(label_file)
                    break
    return invalid_files

invalid_labels = validate_dataset_coordinates(model_label_dir)
if invalid_labels:
    print(f"Found {len(invalid_labels)} files with invalid coordinates:")
    for file in invalid_labels[:10]:  # Print first 10 for brevity
        print(file)
    print("Consider cleaning or correcting these files before proceeding.")
else:
    print("All coordinates in the original dataset are valid.")

def clean_bounding_boxes_on_copy(copy_image_dir, copy_label_dir, min_threshold=0.001, max_threshold=0.9):
    """
    Filters bounding boxes that are too small or too large based on the thresholds.
    Deletes both the images and the corresponding label files if no valid bounding boxes remain,
    but works only on the copy directories.
    """
    cleaned_labels = 0
    total_labels = 0

    # Sort files to ensure matching
    image_files = sorted(os.listdir(copy_image_dir))
    label_files = sorted(os.listdir(copy_label_dir))

    for image_file, label_file in zip(image_files, label_files):
        image_path = os.path.join(copy_image_dir, image_file)
        label_path = os.path.join(copy_label_dir, label_file)

        # Check if the image and label file match
        image_base = os.path.splitext(image_file)[0]
        label_base = os.path.splitext(label_file)[0]

        if image_base != label_base:
            print(f"Warning: {image_file} and {label_file} do not match.")
            continue

        # Read the label file
        with open(label_path, 'r') as f:
            labels = f.readlines()

        new_labels = []
        for label in labels:
            total_labels += 1
            class_idx, x_center, y_center, width, height = map(float, label.strip().split())
            bbox_area = width * height

            # Filter based on the set thresholds
            if min_threshold <= bbox_area <= max_threshold:
                new_labels.append(label)
            else:
                cleaned_labels += 1

        # If there are no valid bounding boxes, delete both the image and the label file
        if len(new_labels) == 0:
            os.remove(image_path)
            os.remove(label_path)
            print(f"Deleted {image_file} and {label_file} due to lack of valid bounding boxes.")
        else:
            # Rewrite the label file only with valid bounding boxes
            with open(label_path, 'w') as f:
                for new_label in new_labels:
                    f.write(new_label)

    print(f"Total bounding boxes processed: {total_labels}")
    print(f"Bounding boxes removed: {cleaned_labels}")

# Run data cleaning on the copy directories
clean_bounding_boxes_on_copy(model_image_dir, model_label_dir, min_threshold=0.001, max_threshold=0.9)

image_cnt = sorted([f for f in os.listdir(model_image_dir) if f.endswith('.jpg') or f.endswith('.png')])
label_cnt = sorted([f for f in os.listdir(model_label_dir) if f.endswith('.txt')])

print(f"Number of images: {len(image_cnt)}")
print(f"Number of labels: {len(label_cnt)}")

def count_class_distribution(label_path, class_names):
    class_counts = Counter()

    # Iterate over label files and count each class occurrence
    label_files = sorted([f for f in os.listdir(label_path) if f.endswith('.txt')])

    for label_file in label_files:
        with open(os.path.join(label_path, label_file), 'r') as f:
            annotations = f.readlines()
            for annot in annotations:
                class_idx = int(annot.split()[0])  # Get the class index
                class_counts[class_idx] += 1  # Increment the count for this class

    # Convert class indices to class names
    sorted_class_counts = sorted(class_counts.items(), key=lambda x: x[1], reverse=True)
    sorted_class_names = [class_names[i] for i, _ in sorted_class_counts]
    sorted_counts = [count for _, count in sorted_class_counts]

    return sorted_class_names, sorted_counts

# Path to label files
label_path = '/content/Stanford-dogs-3/train_model/labels'

# Get the sorted class distribution
sorted_class_names, sorted_counts = count_class_distribution(label_path, class_names)

# Plot the sorted class distribution
plt.figure(figsize=(10, 6))
plt.bar(sorted_class_names, sorted_counts, color='skyblue')
plt.xticks(rotation=45, ha='right')
plt.title('Sorted Class Distribution After Bounding Box Cleaning')
plt.xlabel('Class Label')
plt.ylabel('Frequency')
plt.show()

print(f"Min Count:: {min(sorted_counts)}")
print(f"Max Count:: {max(sorted_counts)}")

import os
import shutil
from collections import defaultdict

def limit_images_per_class(image_dir, label_dir, max_images_per_class=250):
    """Limits the number of images per class in a dataset.

    Args:
        image_dir: Path to the directory containing images.
        label_dir: Path to the directory containing label files.
        max_images_per_class: Maximum number of images to keep per class.
    """
    class_counts = defaultdict(int)

    # Iterate over images and their corresponding labels
    for filename in os.listdir(image_dir):
        if filename.endswith(('.jpg', '.png')):
            image_path = os.path.join(image_dir, filename)
            label_path = os.path.join(label_dir, filename[:-4] + '.txt')

            # Check if label file exists
            if os.path.exists(label_path):
                # Get class from label file
                with open(label_path, 'r') as f:
                    for line in f:
                        class_idx = int(line.split()[0])
                        class_counts[class_idx] += 1

                        # If class count exceeds the limit, delete image and label
                        if class_counts[class_idx] > max_images_per_class:
                            os.remove(image_path)
                            os.remove(label_path)
                            print(f"Deleted {filename} and {filename[:-4] + '.txt'} (class {class_idx})")
                            break  # Move to the next image


# Call the function to limit images per class
limit_images_per_class(model_image_dir, model_label_dir, max_images_per_class=250)

def count_class_distribution(label_path, class_names):
    class_counts = Counter()

    # Iterate over label files and count each class occurrence
    label_files = sorted([f for f in os.listdir(label_path) if f.endswith('.txt')])

    for label_file in label_files:
        with open(os.path.join(label_path, label_file), 'r') as f:
            annotations = f.readlines()
            for annot in annotations:
                class_idx = int(annot.split()[0])  # Get the class index
                class_counts[class_idx] += 1  # Increment the count for this class

    # Convert class indices to class names
    sorted_class_counts = sorted(class_counts.items(), key=lambda x: x[1], reverse=True)
    sorted_class_names = [class_names[i] for i, _ in sorted_class_counts]
    sorted_counts = [count for _, count in sorted_class_counts]

    return sorted_class_names, sorted_counts

# Path to label files
label_path = '/content/Stanford-dogs-3/train_model/labels'

# Get the sorted class distribution
sorted_class_names, sorted_counts = count_class_distribution(label_path, class_names)

# Plot the sorted class distribution
plt.figure(figsize=(10, 6))
plt.bar(sorted_class_names, sorted_counts, color='skyblue')
plt.xticks(rotation=45, ha='right')
plt.title('Sorted Class Distribution After Bounding Box Cleaning')
plt.xlabel('Class Label')
plt.ylabel('Frequency')
plt.show()

image_cnt = sorted([f for f in os.listdir(model_image_dir) if f.endswith('.jpg') or f.endswith('.png')])
label_cnt = sorted([f for f in os.listdir(model_label_dir) if f.endswith('.txt')])

print(f"Number of images: {len(image_cnt)}")
print(f"Number of labels: {len(label_cnt)}")

"""# Modeling"""

dataset_path = '/content/Stanford-dogs-3'
train_old_path = os.path.join(dataset_path, 'train_old')
train_model_path = os.path.join(dataset_path, 'train_model')
train_new_path = os.path.join(dataset_path, 'train')

# First, rename 'train' to 'train_old' if it exists
if os.path.exists(train_new_path):
    os.rename(train_new_path, train_old_path)
    print(f"Renamed 'train' to 'train_old'")

# Then, rename 'train_model' to 'train'
if os.path.exists(train_model_path):
    os.rename(train_model_path, train_new_path)
    print(f"Renamed 'train_model' to 'train'")

from ultralytics import YOLO

# Load YOLOv8 model (you can use a pretrained model like 'yolov8n.pt' or 'yolov8s.pt')
model = YOLO('yolov8n.pt')

results = model.train(data='/content/Stanford-dogs-3/data.yaml',
                      epochs=20,
                      imgsz=608,
                      batch=16,
                      augment=True,
                      )

model_path = '/content/Stanford-dogs-3/20_epoch_model.pt'  # Define the path to save your model
model.save(model_path)
print(f"Model saved at {model_path}")

test_images_path = '/content/Stanford-dogs-3/test/images'
test_image_files = sorted([f for f in os.listdir(test_images_path) if f.endswith('.jpg') or f.endswith('.png')])

# Limit to the first 10 images
test_image_files = test_image_files[:10]

for image_file in test_image_files:
    image_path = os.path.join(test_images_path, image_file)

    # Make predictions
    reslist = model.predict(image_path)

    # Visualize predictions
    for res in reslist[:10]:
        img = res.orig_img  # Original image with predictions
        for box in res.boxes:
            x1, y1, x2, y2 = box.xyxy.cpu().numpy().astype(int)[0]
            cls = int(box.cls[0].cpu().numpy())  # Class index
            conf = box.conf[0].cpu().numpy()  # Confidence score

            # Draw bounding boxes
            cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 0), 2)
            label = f"{model.names[cls]}: {conf:.2f}"
            cv2.putText(img, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)

        # Show the image with predictions
        plt.figure(figsize=(10, 10))
        plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
        plt.axis('off')
        plt.title(image_file)
        plt.show()

# Evaluate the model on the validation set
val_res = model.val(data='/content/Stanford-dogs-3/data.yaml')

import seaborn as sns
import matplotlib.pyplot as plt

# Access the confusion matrix from the validation results
conf_matrix = val_res.confusion_matrix.matrix

# Class names (replace these with actual class names if needed)

# Plot the confusion matrix
plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix, annot=True, fmt=".1f", cmap='Blues', xticklabels=class_names, yticklabels=class_names)
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix')
plt.show()

# Access the confusion matrix
conf_matrix = val_res.confusion_matrix.matrix

# Initialize lists to store precision, recall, and F1-score
precisions = []
recalls = []
f1_scores = []

# Calculate metrics for each class
for i in range(len(class_names)):
    tp = conf_matrix[i, i]  # True positives
    fp = conf_matrix[:, i].sum() - tp  # False positives
    fn = conf_matrix[i, :].sum() - tp  # False negatives

    # Precision and recall calculations
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0

    precisions.append(precision)
    recalls.append(recall)
    f1_scores.append(f1)

# Create a DataFrame to hold the results
results_df = pd.DataFrame({
    'Class': class_names,
    'Precision': precisions,
    'Recall': recalls,
    'F1-Score': f1_scores
})

# Set the Class column as the index
results_df.set_index('Class', inplace=True)

# Print the results in table format
print(results_df)